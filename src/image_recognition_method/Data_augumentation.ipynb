{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d2b05b0-e193-4bc3-8be6-8340a9103d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os                        # Library to interact with the operating system\n",
    "import cv2                       # OpenCV library for computer vision tasks\n",
    "import numpy as np               # NumPy library for numerical operations\n",
    "from sklearn.datasets import fetch_lfw_people   # Function to load the LFW dataset\n",
    "from sklearn.model_selection import train_test_split   # Function to split dataset into train and test subsets\n",
    "\n",
    "# Function to split augmented dataset into training and testing subsets\n",
    "def split_augmented_dataset(augmented_dir, test_size=0.2):\n",
    "    # Get the list of label names (subdirectories in augmented_dir)\n",
    "    label_names = os.listdir(augmented_dir)   # Fetch the names of subdirectories in the augmented dataset\n",
    "    images = []   # Initialize an empty list to store images\n",
    "    labels = []   # Initialize an empty list to store corresponding labels\n",
    "\n",
    "    # Loop through each label and read images from subdirectories\n",
    "    for label_idx, label_name in enumerate(label_names):\n",
    "        label_dir = os.path.join(augmented_dir, label_name)   # Create the full path of the label subdirectory\n",
    "        for image_file in os.listdir(label_dir):   # Loop through each image file in the subdirectory\n",
    "            image_path = os.path.join(label_dir, image_file)   # Create the full path of the image file\n",
    "            image = cv2.imread(image_path)   # Read the image using OpenCV\n",
    "            images.append(image)   # Append the image to the images list\n",
    "            labels.append(label_idx)   # Append the corresponding label to the labels list\n",
    "\n",
    "    images = np.array(images)   # Convert the list of images to a NumPy array\n",
    "    labels = np.array(labels)   # Convert the list of labels to a NumPy array\n",
    "\n",
    "    # Split the images and labels into training and testing subsets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=test_size, random_state=42)   # Split using sklearn's train_test_split\n",
    "    return X_train, X_test, y_train, y_test   # Return the training and testing subsets of images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "194a20d3-f7c6-4128-af6e-de090ce0e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform image augmentation using OpenCV\n",
    "def augment_image(image):\n",
    "    # Check if the image is grayscale (2-dimensional)\n",
    "    if image.ndim == 2:  # Grayscale image\n",
    "        # Convert the grayscale image to RGB format\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    rows, cols, _ = image.shape   # Get the dimensions of the image\n",
    "\n",
    "    # Random rotation between -10 to 10 degrees\n",
    "    random_angle = np.random.randint(-10, 11)   # Generate a random angle between -10 and 10 degrees\n",
    "    M = cv2.getRotationMatrix2D((cols / 2, rows / 2), random_angle, 1)   # Get the rotation matrix for the random angle\n",
    "    augmented_image = cv2.warpAffine(image, M, (cols, rows))   # Apply the rotation to the image using warpAffine\n",
    "\n",
    "    # Random horizontal flipping\n",
    "    if np.random.rand() > 0.5:   # Generate a random number between 0 and 1, and check if it's greater than 0.5\n",
    "        flipped_image = cv2.flip(augmented_image, 1)  # 1 means horizontal flip. Flip the image horizontally\n",
    "    else:\n",
    "        flipped_image = augmented_image   # Keep the image as is (no horizontal flip)\n",
    "\n",
    "    # Random brightness adjustment\n",
    "    brightness_factor = np.random.uniform(0.7, 1.3)   # Generate a random brightness factor between 0.7 and 1.3\n",
    "    hsv_image = cv2.cvtColor(flipped_image, cv2.COLOR_RGB2HSV)   # Convert the RGB image to HSV color space\n",
    "    hsv_image[:, :, 2] = hsv_image[:, :, 2] * brightness_factor   # Adjust the brightness (V channel) by the brightness factor\n",
    "    augmented_image = cv2.cvtColor(hsv_image, cv2.COLOR_HSV2RGB)   # Convert the HSV image back to RGB color space\n",
    "\n",
    "    return augmented_image   # Return the augmented image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29e0f0cf-51ac-4b05-9ccd-cb2f9e178b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_lfw_people_dataset(lfw_people, target_count=10, output_directory='lfw_augmented'):\n",
    "    # Create a new directory for the augmented dataset\n",
    "    augmented_dir = os.path.join(lfw_people.target_names[0], output_directory)   # Create the full path of the augmented directory\n",
    "    os.makedirs(augmented_dir, exist_ok=True)   # Create the augmented directory if it doesn't exist\n",
    "\n",
    "    # Loop through each label in the dataset\n",
    "    for label_idx, label_name in enumerate(lfw_people.target_names):\n",
    "        label_dir = os.path.join(augmented_dir, label_name)   # Create the full path of the label subdirectory\n",
    "        os.makedirs(label_dir, exist_ok=True)   # Create the label subdirectory if it doesn't exist\n",
    "\n",
    "        # Get images belonging to the current label\n",
    "        label_images = lfw_people.images[lfw_people.target == label_idx]   # Fetch the images with the current label\n",
    "\n",
    "        # Check if the label folder already has enough images (>= target_count)\n",
    "        if len(label_images) >= target_count:   # If the label already has enough images\n",
    "            selected_images = label_images[:target_count]   # Select the first target_count number of images\n",
    "        else:\n",
    "            # If the label folder has fewer images, duplicate and augment the existing images\n",
    "            selected_images = []\n",
    "            while len(selected_images) < target_count:\n",
    "                for image in label_images:\n",
    "                    selected_images.append(augment_image(image))   # Augment the image and add to selected_images\n",
    "                    if len(selected_images) == target_count:   # Check if we have enough augmented images\n",
    "                        break\n",
    "\n",
    "        # Perform augmentation for images with count < target_count\n",
    "        for idx, image in enumerate(selected_images):\n",
    "            image_path = os.path.join(label_dir, f'{label_name}_{idx}.png')   # Create the full path of the augmented image\n",
    "            cv2.imwrite(image_path, image)   # Write the augmented image to the specified path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ae7908c-7a00-40c9-8eb2-18cc346a144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LFW dataset\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=1, resize=0.4)   # Fetch the LFW dataset with specified parameters\n",
    "\n",
    "# Augment the LFW dataset\n",
    "augment_lfw_people_dataset(lfw_people, target_count=10)   # Augment the dataset with 10 images per class\n",
    "\n",
    "# Split the augmented dataset into training and testing subsets\n",
    "augmented_dir = os.path.join(lfw_people.target_names[0], 'lfw_augmented')   # Create the full path of the augmented directory\n",
    "X_train, X_test, y_train, y_test = split_augmented_dataset(augmented_dir, test_size=0.2)   # Split the dataset into train and test subsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c749567f-0f83-42de-b77c-740f63f5b913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "353b480b-4243-4e93-8e6e-e8edeb38ea26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd2d5ea65d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = plt.imread(\"./AJ Cook/lfw_augmented/Angie Martinez/Angie Martinez_4.png\")\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8e98dc-70e2-4aeb-8749-e986795943c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-02 20:19:30.461429: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1150/1150 [==============================] - 44s 38ms/step - loss: 8.6736 - accuracy: 5.4358e-05 - val_loss: 8.6336 - val_accuracy: 4.3483e-04\n",
      "Epoch 2/40\n",
      " 726/1150 [=================>............] - ETA: 15s - loss: 8.4960 - accuracy: 9.4697e-04"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# Convert the labels to one-hot encoded format (categorical)\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train_cat = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# Normalize the pixel values to the range [0, 1]\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(50, 37, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 40\n",
    "model.fit(X_train, y_train_cat, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "accuracy = accuracy_score(y_test, y_pred_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9e8c56-0753-4854-8aa4-8b6ab4c1939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d028c0c-ac36-4298-aae8-fd362198c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report_str = classification_report(y_test, y_pred_labels, target_names=y_train)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report_str)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
