{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4211001f-d29f-49fe-b826-6afd6216c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os                        # Library to interact with the operating system\n",
    "import cv2                       # OpenCV library for computer vision tasks\n",
    "import numpy as np               # NumPy library for numerical operations\n",
    "from sklearn.datasets import fetch_lfw_people   # Function to load the LFW dataset\n",
    "from sklearn.model_selection import train_test_split   # Function to split dataset into train and test subsets\n",
    "\n",
    "# Function to split augmented dataset into training and testing subsets\n",
    "def split_augmented_dataset(augmented_dir, test_size=0.2):\n",
    "    # Get the list of label names (subdirectories in augmented_dir)\n",
    "    label_names = os.listdir(augmented_dir)   # Fetch the names of subdirectories in the augmented dataset\n",
    "    images = []   # Initialize an empty list to store images\n",
    "    labels = []   # Initialize an empty list to store corresponding labels\n",
    "\n",
    "    # Loop through each label and read images from subdirectories\n",
    "    for label_idx, label_name in enumerate(label_names):\n",
    "        label_dir = os.path.join(augmented_dir, label_name)   # Create the full path of the label subdirectory\n",
    "        for image_file in os.listdir(label_dir):   # Loop through each image file in the subdirectory\n",
    "            image_path = os.path.join(label_dir, image_file)   # Create the full path of the image file\n",
    "            image = cv2.imread(image_path)   # Read the image using OpenCV\n",
    "            try:\n",
    "                if image.ndim == 2:  # Grayscale image\n",
    "                    # Convert the grayscale image to RGB format\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "                images.append(image)   # Append the image to the images list\n",
    "                labels.append(label_idx)   # Append the corresponding label to the labels list\n",
    "            except:\n",
    "                pass\n",
    "    images = np.stack(images, axis=0)\n",
    "    # images = np.array(images)   # Convert the list of images to a NumPy array\n",
    "    labels = np.array(labels)   # Convert the list of labels to a NumPy array\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "\n",
    "    # Split the images and labels into training and testing subsets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=test_size, random_state=42)   # Split using sklearn's train_test_split\n",
    "    return X_train, X_test, y_train, y_test   # Return the training and testing subsets of images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7bad685-ae94-43cc-9c85-09498428611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform image augmentation using OpenCV\n",
    "def augment_image(image):\n",
    "    # Check if the image is grayscale (2-dimensional)\n",
    "    if image.ndim == 2:  # Grayscale image\n",
    "        # Convert the grayscale image to RGB format\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    rows, cols, _ = image.shape   # Get the dimensions of the image\n",
    "\n",
    "    # Random rotation between -10 to 10 degrees\n",
    "    random_angle = np.random.randint(-10, 11)   # Generate a random angle between -10 and 10 degrees\n",
    "    M = cv2.getRotationMatrix2D((cols / 2, rows / 2), random_angle, 1)   # Get the rotation matrix for the random angle\n",
    "    augmented_image = cv2.warpAffine(image, M, (cols, rows))   # Apply the rotation to the image using warpAffine\n",
    "\n",
    "    # Random horizontal flipping\n",
    "    if np.random.rand() > 0.5:   # Generate a random number between 0 and 1, and check if it's greater than 0.5\n",
    "        flipped_image = cv2.flip(augmented_image, 1)  # 1 means horizontal flip. Flip the image horizontally\n",
    "    else:\n",
    "        flipped_image = augmented_image   # Keep the image as is (no horizontal flip)\n",
    "\n",
    "    # Random brightness adjustment\n",
    "    brightness_factor = np.random.uniform(0.7, 1.3)   # Generate a random brightness factor between 0.7 and 1.3\n",
    "    hsv_image = cv2.cvtColor(flipped_image, cv2.COLOR_RGB2HSV)   # Convert the RGB image to HSV color space\n",
    "    hsv_image[:, :, 2] = hsv_image[:, :, 2] * brightness_factor   # Adjust the brightness (V channel) by the brightness factor\n",
    "    augmented_image = cv2.cvtColor(hsv_image, cv2.COLOR_HSV2RGB)   # Convert the HSV image back to RGB color space\n",
    "\n",
    "    return augmented_image   # Return the augmented image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e143386-2750-40a8-b87b-4ed61403c970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_lfw_people_dataset(lfw_people, target_count=10, output_directory='lfw_augmented'):\n",
    "    # Create a new directory for the augmented dataset\n",
    "    augmented_dir = os.path.join(lfw_people.target_names[0], output_directory)   # Create the full path of the augmented directory\n",
    "    os.makedirs(augmented_dir, exist_ok=True)   # Create the augmented directory if it doesn't exist\n",
    "\n",
    "    # Loop through each label in the dataset\n",
    "    for label_idx, label_name in enumerate(lfw_people.target_names):\n",
    "        label_dir = os.path.join(augmented_dir, label_name)   # Create the full path of the label subdirectory\n",
    "        os.makedirs(label_dir, exist_ok=True)   # Create the label subdirectory if it doesn't exist\n",
    "\n",
    "        # Get images belonging to the current label\n",
    "        label_images = lfw_people.images[lfw_people.target == label_idx]   # Fetch the images with the current label\n",
    "\n",
    "        # Check if the label folder already has enough images (>= target_count)\n",
    "        if len(label_images) >= target_count:   # If the label already has enough images\n",
    "            selected_images = label_images[:target_count]   # Select the first target_count number of images\n",
    "        else:\n",
    "            # If the label folder has fewer images, duplicate and augment the existing images\n",
    "            selected_images = []\n",
    "            while len(selected_images) < target_count:\n",
    "                for image in label_images:\n",
    "                    selected_images.append(image)   # Augment the image and add to selected_images\n",
    "                    if len(selected_images) == target_count:   # Check if we have enough augmented images\n",
    "                        break\n",
    "\n",
    "        # Perform augmentation for images with count < target_count\n",
    "        for idx, image in enumerate(selected_images):\n",
    "            image_path = os.path.join(label_dir, f'{label_name}_{idx}.png')   # Create the full path of the augmented image\n",
    "            cv2.imwrite(image_path, image)   # Write the augmented image to the specified path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "742ab514-6311-47bb-ad2e-7d2dfe772580",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57490, 50, 37, 3)\n",
      "(57490,)\n"
     ]
    }
   ],
   "source": [
    "# Load the LFW dataset\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=1, resize=0.4)   # Fetch the LFW dataset with specified parameters\n",
    "\n",
    "   # Augment the dataset with 10 images per class\n",
    "\n",
    "# Split the augmented dataset into training and testing subsets\n",
    "augmented_dir = os.path.join(lfw_people.target_names[0], 'lfw_augmented')   # Create the full path of the augmented directory\n",
    "if os.path.exists(augmented_dir):\n",
    "    # Augment the LFW dataset\n",
    "    augment_lfw_people_dataset(lfw_people, target_count=10)\n",
    "    X_train, X_test, y_train, y_test = split_augmented_dataset(augmented_dir, test_size=0.2)   # Split the dataset into train and test subsets\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = split_augmented_dataset(augmented_dir, test_size=0.2)   # Split the dataset into train and test subsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f4a1526-9903-43cb-bc58-5c67269c2c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45992, 50, 37, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c36ce51a-a893-4b9e-aaf9-15da53c72eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11498, 50, 37, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78424a5f-7cf8-4f56-b062-d094063553f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train.reshape(-1, 50 * 37 * 3)\n",
    "# X_test = X_test.reshape(-1, 50 * 37 * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fb61ca1-3853-44a7-a576-f7f2d7eaa78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog, local_binary_pattern\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from skimage import exposure\n",
    "  \n",
    "def scale_invariant_feature_transform(image):\n",
    "    \n",
    "    # Convert the image depth to CV_8U\n",
    "    image8bit = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n",
    "    \n",
    "    #reading image\n",
    "    gray1 = cv2.cvtColor(image8bit, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #keypoints\n",
    "    #keypoints``\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints_1, descriptors_1 = sift.detectAndCompute(gray1, None)\n",
    "\n",
    "    img_1 = cv2.drawKeypoints(gray1,keypoints_1,image)\n",
    "    return img_1\n",
    "    \n",
    "def local_binary_patterns(image):\n",
    "    \n",
    "    # settings for LBP\n",
    "    radius = 3\n",
    "    n_points = 8 * radius\n",
    "    \n",
    "    # Convert the image depth to CV_8U\n",
    "    image8bit = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n",
    "\n",
    "    # Convert the array to grayscale and reshape it to (height, width)\n",
    "    gray1 = cv2.cvtColor(image8bit, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    lbp = local_binary_pattern(gray1, n_points, radius, method = \"uniform\")\n",
    "\n",
    "    return lbp\n",
    "\n",
    "# Function to extract combined features (HOG, SIFT, and LBP) from an image\n",
    "def extract_combined_features(image):\n",
    "    sift_features = scale_invariant_feature_transform(image)\n",
    "    lbp_features = local_binary_patterns(image)\n",
    "    \n",
    "    sift_features = sift_features.flatten()\n",
    "    lbp_features = lbp_features.flatten()\n",
    "    \n",
    "    merged_feature = np.hstack((sift_features,lbp_features))\n",
    "    return merged_feature\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18cbbb1c-9d26-4e9a-b1cf-a597e9eac20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract combined features from the training and testing images\n",
    "X_train_features = np.array([extract_combined_features(image) for image in X_train])\n",
    "X_test_features = np.array([extract_combined_features(image) for image in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1cdbc48-0289-4139-9a46-fd1c24a1ebf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45992, 7400)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15497f15-e6a2-4589-8e1b-ccbf1a31a2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.decomposition import PCA\n",
    "def pca_implementation(X_train, X_test):\n",
    "    # Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled\n",
    "    # dataset): unsupervised feature extraction / dimensionality reduction\n",
    "    n_components = 60\n",
    "\n",
    "    print(\"Extracting the top %d eigenfaces from %d faces\"\n",
    "          % (n_components, X_train.shape[0]))\n",
    "    t0 = time()\n",
    "    pca = PCA(n_components=n_components, svd_solver='randomized',\n",
    "              whiten=True).fit(X_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "    print(\"Projecting the input data on the eigenfaces orthonormal basis\")\n",
    "    t0 = time()\n",
    "    X_train_pca = pca.transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    return X_train_pca, X_test_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05592b3b-d510-4167-b2b2-f721ee82b616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the top 60 eigenfaces from 45992 faces\n",
      "done in 19.738s\n",
      "Projecting the input data on the eigenfaces orthonormal basis\n",
      "done in 2.897s\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test= pca_implementation(X_train_features, X_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c7f7d1-d7e1-4c92-84dc-b69bf18f828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an XGBoost model\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508b7911-0903-4c74-8ad2-5a26a8a11879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the XGBoost model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate classification report and accuracy score\n",
    "classification_report_output = classification_report(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report_output)\n",
    "print(\"Accuracy Score:\", accuracy)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498b2062-6e23-4779-9fde-a1e8f1750ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
