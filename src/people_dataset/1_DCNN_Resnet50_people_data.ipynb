{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d2b05b0-e193-4bc3-8be6-8340a9103d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os                        # Library to interact with the operating system\n",
    "import cv2                       # OpenCV library for computer vision tasks\n",
    "import numpy as np               # NumPy library for numerical operations\n",
    "from sklearn.datasets import fetch_lfw_people   # Function to load the LFW dataset\n",
    "from sklearn.model_selection import train_test_split   # Function to split dataset into train and test subsets\n",
    "\n",
    "# Function to split augmented dataset into training and testing subsets\n",
    "def split_augmented_dataset(augmented_dir, test_size=0.2):\n",
    "    # Get the list of label names (subdirectories in augmented_dir)\n",
    "    label_names = os.listdir(augmented_dir)   # Fetch the names of subdirectories in the augmented dataset\n",
    "    images = []   # Initialize an empty list to store images\n",
    "    labels = []   # Initialize an empty list to store corresponding labels\n",
    "\n",
    "    # Loop through each label and read images from subdirectories\n",
    "    for label_idx, label_name in enumerate(label_names):\n",
    "        label_dir = os.path.join(augmented_dir, label_name)   # Create the full path of the label subdirectory\n",
    "        for image_file in os.listdir(label_dir):   # Loop through each image file in the subdirectory\n",
    "            image_path = os.path.join(label_dir, image_file)   # Create the full path of the image file\n",
    "            image = cv2.imread(image_path)   # Read the image using OpenCV\n",
    "            images.append(image)   # Append the image to the images list\n",
    "            labels.append(label_idx)   # Append the corresponding label to the labels list\n",
    "\n",
    "    images = np.array(images)   # Convert the list of images to a NumPy array\n",
    "    labels = np.array(labels)   # Convert the list of labels to a NumPy array\n",
    "\n",
    "    # Split the images and labels into training and testing subsets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=test_size, random_state=42)   # Split using sklearn's train_test_split\n",
    "    return X_train, X_test, y_train, y_test   # Return the training and testing subsets of images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "194a20d3-f7c6-4128-af6e-de090ce0e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform image augmentation using OpenCV\n",
    "def augment_image(image):\n",
    "    # Check if the image is grayscale (2-dimensional)\n",
    "    if image.ndim == 2:  # Grayscale image\n",
    "        # Convert the grayscale image to RGB format\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    rows, cols, _ = image.shape   # Get the dimensions of the image\n",
    "\n",
    "    # Random rotation between -10 to 10 degrees\n",
    "    random_angle = np.random.randint(-10, 11)   # Generate a random angle between -10 and 10 degrees\n",
    "    M = cv2.getRotationMatrix2D((cols / 2, rows / 2), random_angle, 1)   # Get the rotation matrix for the random angle\n",
    "    augmented_image = cv2.warpAffine(image, M, (cols, rows))   # Apply the rotation to the image using warpAffine\n",
    "\n",
    "    # Random horizontal flipping\n",
    "    if np.random.rand() > 0.5:   # Generate a random number between 0 and 1, and check if it's greater than 0.5\n",
    "        flipped_image = cv2.flip(augmented_image, 1)  # 1 means horizontal flip. Flip the image horizontally\n",
    "    else:\n",
    "        flipped_image = augmented_image   # Keep the image as is (no horizontal flip)\n",
    "\n",
    "    # Random brightness adjustment\n",
    "    brightness_factor = np.random.uniform(0.7, 1.3)   # Generate a random brightness factor between 0.7 and 1.3\n",
    "    hsv_image = cv2.cvtColor(flipped_image, cv2.COLOR_RGB2HSV)   # Convert the RGB image to HSV color space\n",
    "    hsv_image[:, :, 2] = hsv_image[:, :, 2] * brightness_factor   # Adjust the brightness (V channel) by the brightness factor\n",
    "    augmented_image = cv2.cvtColor(hsv_image, cv2.COLOR_HSV2RGB)   # Convert the HSV image back to RGB color space\n",
    "\n",
    "    return augmented_image   # Return the augmented image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a98dd293-4385-4c7c-817b-e60ebc8a00ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29e0f0cf-51ac-4b05-9ccd-cb2f9e178b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_lfw_people_dataset(lfw_people, target_count=10, output_directory='lfw_augmented'):\n",
    "    # Create a new directory for the augmented dataset\n",
    "    augmented_dir = os.path.join(lfw_people.target_names[0], output_directory)   # Create the full path of the augmented directory\n",
    "    os.makedirs(augmented_dir, exist_ok=True)   # Create the augmented directory if it doesn't exist\n",
    "\n",
    "    # Loop through each label in the dataset\n",
    "    for label_idx, label_name in enumerate(lfw_people.target_names):\n",
    "        label_dir = os.path.join(augmented_dir, label_name)   # Create the full path of the label subdirectory\n",
    "        os.makedirs(label_dir, exist_ok=True)   # Create the label subdirectory if it doesn't exist\n",
    "\n",
    "        # Get images belonging to the current label\n",
    "        label_images = lfw_people.images[lfw_people.target == label_idx]   # Fetch the images with the current label\n",
    "\n",
    "        # Check if the label folder already has enough images (>= target_count)\n",
    "        if len(label_images) >= target_count:   # If the label already has enough images\n",
    "            selected_images = label_images[:target_count]   # Select the first target_count number of images\n",
    "        else:\n",
    "            # If the label folder has fewer images, duplicate and augment the existing images\n",
    "            selected_images = []\n",
    "            while len(selected_images) < target_count:\n",
    "                for image in label_images:\n",
    "                    selected_images.append(augment_image(image))   # Augment the image and add to selected_images\n",
    "                    if len(selected_images) == target_count:   # Check if we have enough augmented images\n",
    "                        break\n",
    "\n",
    "        # Perform augmentation for images with count < target_count\n",
    "        for idx, image in enumerate(selected_images):\n",
    "            image_path = os.path.join(label_dir, f'{label_name}_{idx}.png')   # Create the full path of the augmented image\n",
    "            # image = cv2.normalize(image, None, 0, 1.0, cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "            image = cv2.convertScaleAbs(image, alpha=(255.0))\n",
    "            cv2.imwrite(image_path, image)\n",
    "            # plt.imsave(image_path, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ae7908c-7a00-40c9-8eb2-18cc346a144c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the LFW dataset\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=1, resize=0.4)   # Fetch the LFW dataset with specified parameters\n",
    "\n",
    "# Augment the LFW dataset\n",
    "augment_lfw_people_dataset(lfw_people, target_count=10)   # Augment the dataset with 10 images per class\n",
    "\n",
    "# Split the augmented dataset into training and testing subsets\n",
    "augmented_dir = os.path.join(lfw_people.target_names[0], 'lfw_augmented')   # Create the full path of the augmented directory\n",
    "X_train, X_test, y_train, y_test = split_augmented_dataset(augmented_dir, test_size=0.2)   # Split the dataset into train and test subsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c749567f-0f83-42de-b77c-740f63f5b913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "353b480b-4243-4e93-8e6e-e8edeb38ea26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc980636d10>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = plt.imread(\"./AJ Cook/lfw_augmented/Angie Martinez/Angie Martinez_4.png\")\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc8e98dc-70e2-4aeb-8749-e986795943c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 02:39:52.598212: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-03 02:39:52.610765: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-03 02:39:52.612407: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-03 02:39:52.614501: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-03 02:39:52.615621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-03 02:39:52.617256: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-03 02:39:52.618860: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-03 02:39:53.262856: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-03 02:39:53.264733: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-03 02:39:53.266355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-03 02:39:53.267912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13582 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# Convert the labels to one-hot encoded format (categorical)\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train_cat = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# Normalize the pixel values to the range [0, 1]\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(50, 37, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 40\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34a15e42-f31e-420d-9723-63f11babbfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 02:39:56.668033: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8200\n",
      "2023-08-03 02:39:58.092184: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7fc32f2f7b90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-03 02:39:58.092230: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2023-08-03 02:39:58.099481: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-08-03 02:39:58.258911: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1150/1150 [==============================] - 11s 6ms/step - loss: 8.6717 - accuracy: 1.9025e-04 - val_loss: 8.6312 - val_accuracy: 0.0015\n",
      "Epoch 2/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 8.2488 - accuracy: 0.0039 - val_loss: 7.7909 - val_accuracy: 0.0125\n",
      "Epoch 3/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 7.0136 - accuracy: 0.0278 - val_loss: 6.4642 - val_accuracy: 0.0922\n",
      "Epoch 4/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 5.9216 - accuracy: 0.0840 - val_loss: 5.3422 - val_accuracy: 0.2233\n",
      "Epoch 5/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 5.0828 - accuracy: 0.1572 - val_loss: 4.4375 - val_accuracy: 0.3522\n",
      "Epoch 6/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 4.4952 - accuracy: 0.2163 - val_loss: 3.8517 - val_accuracy: 0.4431\n",
      "Epoch 7/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 4.0140 - accuracy: 0.2785 - val_loss: 3.4207 - val_accuracy: 0.5126\n",
      "Epoch 8/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 3.6380 - accuracy: 0.3310 - val_loss: 3.0679 - val_accuracy: 0.5690\n",
      "Epoch 9/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 3.3433 - accuracy: 0.3763 - val_loss: 2.8602 - val_accuracy: 0.6041\n",
      "Epoch 10/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 3.0470 - accuracy: 0.4238 - val_loss: 2.6039 - val_accuracy: 0.6395\n",
      "Epoch 11/40\n",
      "1150/1150 [==============================] - 6s 6ms/step - loss: 2.8078 - accuracy: 0.4585 - val_loss: 2.4304 - val_accuracy: 0.6711\n",
      "Epoch 12/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 2.6108 - accuracy: 0.4933 - val_loss: 2.3011 - val_accuracy: 0.6875\n",
      "Epoch 13/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 2.4179 - accuracy: 0.5243 - val_loss: 2.1480 - val_accuracy: 0.7093\n",
      "Epoch 14/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 2.2405 - accuracy: 0.5546 - val_loss: 2.0433 - val_accuracy: 0.7287\n",
      "Epoch 15/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 2.0940 - accuracy: 0.5784 - val_loss: 1.9727 - val_accuracy: 0.7400\n",
      "Epoch 16/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 1.9606 - accuracy: 0.6007 - val_loss: 1.9154 - val_accuracy: 0.7467\n",
      "Epoch 17/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 1.8378 - accuracy: 0.6228 - val_loss: 1.8637 - val_accuracy: 0.7553\n",
      "Epoch 18/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 1.7326 - accuracy: 0.6411 - val_loss: 1.7873 - val_accuracy: 0.7655\n",
      "Epoch 19/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 1.6540 - accuracy: 0.6580 - val_loss: 1.7829 - val_accuracy: 0.7716\n",
      "Epoch 20/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 1.5500 - accuracy: 0.6729 - val_loss: 1.7097 - val_accuracy: 0.7796\n",
      "Epoch 21/40\n",
      "1150/1150 [==============================] - 6s 6ms/step - loss: 1.4792 - accuracy: 0.6868 - val_loss: 1.6454 - val_accuracy: 0.7903\n",
      "Epoch 22/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 1.4169 - accuracy: 0.6974 - val_loss: 1.6556 - val_accuracy: 0.7910\n",
      "Epoch 23/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 1.3364 - accuracy: 0.7127 - val_loss: 1.6054 - val_accuracy: 0.7986\n",
      "Epoch 24/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 1.2759 - accuracy: 0.7242 - val_loss: 1.6092 - val_accuracy: 0.7987\n",
      "Epoch 25/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 1.2175 - accuracy: 0.7332 - val_loss: 1.5662 - val_accuracy: 0.8049\n",
      "Epoch 26/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 1.1610 - accuracy: 0.7437 - val_loss: 1.5852 - val_accuracy: 0.8081\n",
      "Epoch 27/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 1.1252 - accuracy: 0.7503 - val_loss: 1.5574 - val_accuracy: 0.8112\n",
      "Epoch 28/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 1.0886 - accuracy: 0.7576 - val_loss: 1.5613 - val_accuracy: 0.8156\n",
      "Epoch 29/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 1.0421 - accuracy: 0.7664 - val_loss: 1.5259 - val_accuracy: 0.8170\n",
      "Epoch 30/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 1.0270 - accuracy: 0.7696 - val_loss: 1.5344 - val_accuracy: 0.8211\n",
      "Epoch 31/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 0.9653 - accuracy: 0.7811 - val_loss: 1.5328 - val_accuracy: 0.8245\n",
      "Epoch 32/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 0.9423 - accuracy: 0.7821 - val_loss: 1.4974 - val_accuracy: 0.8248\n",
      "Epoch 33/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 0.9090 - accuracy: 0.7908 - val_loss: 1.4801 - val_accuracy: 0.8304\n",
      "Epoch 34/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 0.8769 - accuracy: 0.7957 - val_loss: 1.5239 - val_accuracy: 0.8314\n",
      "Epoch 35/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 0.8523 - accuracy: 0.8008 - val_loss: 1.4732 - val_accuracy: 0.8343\n",
      "Epoch 36/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 0.8331 - accuracy: 0.8050 - val_loss: 1.5076 - val_accuracy: 0.8324\n",
      "Epoch 37/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 0.8071 - accuracy: 0.8101 - val_loss: 1.5082 - val_accuracy: 0.8410\n",
      "Epoch 38/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 0.7762 - accuracy: 0.8156 - val_loss: 1.5367 - val_accuracy: 0.8366\n",
      "Epoch 39/40\n",
      "1150/1150 [==============================] - 6s 5ms/step - loss: 0.7540 - accuracy: 0.8186 - val_loss: 1.5213 - val_accuracy: 0.8420\n",
      "Epoch 40/40\n",
      "1150/1150 [==============================] - 6s 6ms/step - loss: 0.7451 - accuracy: 0.8207 - val_loss: 1.5306 - val_accuracy: 0.8426\n",
      "360/360 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "with tf.device('/GPU:0'):\n",
    "    history = model.fit(X_train, y_train_cat, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "accuracy = accuracy_score(y_test, y_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac9e8c56-0753-4854-8aa4-8b6ab4c1939b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.38%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b7f9a497-fc7b-4d6f-b27a-910979879be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.ResNet50(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(50, 37, 3),\n",
    "    include_top=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fb36eb59-da93-4a8d-a151-60e31c75057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "55e0d253-f395-4966-bca4-e5c1ecbe6df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(50, 37, 3))\n",
    "# We make sure that the base_model is running in inference mode here,\n",
    "# by passing `training=False`. This is important for fine-tuning, as you will\n",
    "# learn in a few paragraphs.\n",
    "x = base_model(inputs, training=True)\n",
    "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "outputs = Dense(num_classes, activation='softmax')(x)\n",
    "# outputs = keras.layers.Dense(1)(x)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bc516990-9484-4fe8-888c-7595c78d3057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783589be-b546-4b85-8b59-4b901a6c34b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1150/1150 [==============================] - 92s 54ms/step - loss: 8.3444 - accuracy: 6.7948e-04 - val_loss: 7.7121 - val_accuracy: 0.0016\n",
      "Epoch 2/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 6.6918 - accuracy: 0.0155 - val_loss: 6.4077 - val_accuracy: 0.0240\n",
      "Epoch 3/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 5.1617 - accuracy: 0.0890 - val_loss: 5.1945 - val_accuracy: 0.1143\n",
      "Epoch 4/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 3.5981 - accuracy: 0.2895 - val_loss: 3.6577 - val_accuracy: 0.3615\n",
      "Epoch 5/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 2.1936 - accuracy: 0.5564 - val_loss: 2.5769 - val_accuracy: 0.5733\n",
      "Epoch 6/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 1.3021 - accuracy: 0.7298 - val_loss: 1.9548 - val_accuracy: 0.7013\n",
      "Epoch 7/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.8005 - accuracy: 0.8292 - val_loss: 1.6332 - val_accuracy: 0.7539\n",
      "Epoch 8/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.5219 - accuracy: 0.8766 - val_loss: 1.4977 - val_accuracy: 0.7798\n",
      "Epoch 9/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.3343 - accuracy: 0.9178 - val_loss: 1.4135 - val_accuracy: 0.8057\n",
      "Epoch 10/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.2268 - accuracy: 0.9409 - val_loss: 1.3405 - val_accuracy: 0.8251\n",
      "Epoch 11/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.1583 - accuracy: 0.9581 - val_loss: 1.3224 - val_accuracy: 0.8359\n",
      "Epoch 12/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.1305 - accuracy: 0.9644 - val_loss: 1.3248 - val_accuracy: 0.8411\n",
      "Epoch 13/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.0966 - accuracy: 0.9736 - val_loss: 1.2863 - val_accuracy: 0.8577\n",
      "Epoch 14/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.0924 - accuracy: 0.9734 - val_loss: 1.3832 - val_accuracy: 0.8465\n",
      "Epoch 15/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.0801 - accuracy: 0.9774 - val_loss: 1.3090 - val_accuracy: 0.8511\n",
      "Epoch 16/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.0606 - accuracy: 0.9829 - val_loss: 1.3181 - val_accuracy: 0.8597\n",
      "Epoch 17/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.0621 - accuracy: 0.9819 - val_loss: 1.3067 - val_accuracy: 0.8625\n",
      "Epoch 18/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.0481 - accuracy: 0.9863 - val_loss: 1.2947 - val_accuracy: 0.8651\n",
      "Epoch 19/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.0479 - accuracy: 0.9863 - val_loss: 1.3526 - val_accuracy: 0.8627\n",
      "Epoch 20/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.0451 - accuracy: 0.9871 - val_loss: 1.3240 - val_accuracy: 0.8669\n",
      "Epoch 21/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.0421 - accuracy: 0.9881 - val_loss: 1.3352 - val_accuracy: 0.8663\n",
      "Epoch 22/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.0357 - accuracy: 0.9896 - val_loss: 1.3503 - val_accuracy: 0.8734\n",
      "Epoch 23/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.0305 - accuracy: 0.9912 - val_loss: 1.3244 - val_accuracy: 0.8726\n",
      "Epoch 24/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.0333 - accuracy: 0.9909 - val_loss: 1.3869 - val_accuracy: 0.8654\n",
      "Epoch 25/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.0347 - accuracy: 0.9899 - val_loss: 1.3791 - val_accuracy: 0.8646\n",
      "Epoch 26/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.0261 - accuracy: 0.9927 - val_loss: 1.3115 - val_accuracy: 0.8796\n",
      "Epoch 27/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.0259 - accuracy: 0.9925 - val_loss: 1.3334 - val_accuracy: 0.8751\n",
      "Epoch 28/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.0207 - accuracy: 0.9941 - val_loss: 1.3498 - val_accuracy: 0.8761\n",
      "Epoch 29/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.0246 - accuracy: 0.9927 - val_loss: 1.3153 - val_accuracy: 0.8847\n",
      "Epoch 30/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.0245 - accuracy: 0.9931 - val_loss: 1.3657 - val_accuracy: 0.8764\n",
      "Epoch 31/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.0205 - accuracy: 0.9945 - val_loss: 1.3690 - val_accuracy: 0.8809\n",
      "Epoch 32/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.0147 - accuracy: 0.9957 - val_loss: 1.3969 - val_accuracy: 0.8724\n",
      "Epoch 33/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.0193 - accuracy: 0.9940 - val_loss: 1.3684 - val_accuracy: 0.8753\n",
      "Epoch 34/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.0140 - accuracy: 0.9961 - val_loss: 1.3755 - val_accuracy: 0.8819\n",
      "Epoch 35/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.0146 - accuracy: 0.9961 - val_loss: 1.4269 - val_accuracy: 0.8774\n",
      "Epoch 36/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.0184 - accuracy: 0.9946 - val_loss: 1.3839 - val_accuracy: 0.8794\n",
      "Epoch 37/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.0127 - accuracy: 0.9965 - val_loss: 1.4283 - val_accuracy: 0.8756\n",
      "Epoch 38/40\n",
      "1150/1150 [==============================] - 60s 52ms/step - loss: 0.0163 - accuracy: 0.9957 - val_loss: 1.3858 - val_accuracy: 0.8829\n",
      "Epoch 39/40\n",
      "1149/1150 [============================>.] - ETA: 0s - loss: 0.0113 - accuracy: 0.9967"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "with tf.device('/GPU:0'):\n",
    "    history = model.fit(X_train, y_train_cat, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "accuracy = accuracy_score(y_test, y_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "73b6dcec-6ada-4bb0-9586-7b9b67af64e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.62%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d028c0c-ac36-4298-aae8-fd362198c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report_str = classification_report(y_test, y_pred_labels, target_names=y_train)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report_str)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
